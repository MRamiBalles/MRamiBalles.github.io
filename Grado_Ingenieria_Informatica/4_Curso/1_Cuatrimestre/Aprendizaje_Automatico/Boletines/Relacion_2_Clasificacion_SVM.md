# AA - Relaci贸n 2: Modelos de Clasificaci贸n y Clasificadores Nucleares (Oficial UHU)

La clasificaci贸n es una tarea de aprendizaje supervisado donde el objetivo es predecir una etiqueta discreta a partir de un vector de caracter铆sticas.

## 1. Regresi贸n Log铆stica
A pesar de su nombre, es un modelo de clasificaci贸n binaria que utiliza la funci贸n sigmoide: $g(z) = \frac{1}{1 + e^{-z}}$.
- **Interpretaci贸n**: Proporciona el grado de confianza (probabilidad) de que una muestra pertenezca a la clase positiva.

## 2. M谩quinas de Vector de Soporte (SVM)
Buscan el hiperplano que maximiza el margen entre clases.
- **Vectores de Soporte**: Muestras que definen el margen.
- **Truco del Kernel (Kernel Trick)**: Permite proyectar los datos a un espacio de mayor dimensi贸n para resolver problemas no lineales sin calcular expl铆citamente dicha proyecci贸n.

##  Ejercicio Te贸rico: Bias vs Variance
Explique el concepto de Sobreajuste (*Overfitting*) en t茅rminos de sesgo y varianza.
*Respuesta*: 
- **Alto Sesgo (Underfitting)**: El modelo es demasiado simple y no captura la estructura de los datos (ej. recta para datos parab贸licos). Error alto en entrenamiento y test.
- **Alta Varianza (Overfitting)**: El modelo es demasiado complejo y "memoriza" el ruido del entrenamiento. Error muy bajo en entrenamiento pero muy alto en datos nuevos (test).

## 3. Regularizaci贸n ($L_1, L_2$)
Tecnica para combatir el sobreajuste penalizando los pesos altos en la funci贸n de coste.
- **Ridge ($L_2$)**: A帽ade $\lambda \sum \theta_j^2$.
- **Lasso ($L_1$)**: A帽ade $\lambda \sum |\theta_j|$. til para la selecci贸n de atributos ya que tiende a anular pesos no importantes.
