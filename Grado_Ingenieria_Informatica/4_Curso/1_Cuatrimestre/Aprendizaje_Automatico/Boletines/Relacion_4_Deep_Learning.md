# AA - Relaci贸n 4: Redes Neuronales y Deep Learning (Oficial UHU)

El Deep Learning es una subcategor铆a del Machine Learning basada en Redes Neuronales Artificiales con m煤ltiples capas ocultas, capaces de aprender representaciones jer谩rquicas de los datos.

## 1. El Perceptr贸n Multicapa (MLP)
Consiste en una capa de entrada, una o m谩s capas ocultas y una capa de salida. Cada neurona realiza una transformaci贸n af铆n seguida de una **Funci贸n de Activaci贸n**.

### Funciones de Activaci贸n Comunes
- **Sigmoide**: $\sigma(z) = \frac{1}{1+e^{-z}}$. Rango (0, 1). Sufre de desvanecimiento de gradiente.
- **ReLU (Rectified Linear Unit)**: $f(z) = \max(0, z)$. Est谩ndar en capas ocultas por su eficiencia.
- **Softmax**: Utilizada en la capa de salida para clasificaci贸n multiclase (proporciona una distribuci贸n de probabilidad).

## 2. El Algoritmo de Retropropagaci贸n (Backpropagation)
Es la t茅cnica fundamental para entrenar redes. Utiliza la **Regla de la Cadena** para calcular el gradiente de la funci贸n de p茅rdida respecto a cada peso de la red.

###  Trazado Simplificado (Matem谩ticas del Gradiente)
Para un peso $w_{ij}$ en la capa $l$, el ajuste sigue la regla:
$$\Delta w_{ij} = - \alpha \frac{\partial Loss}{\partial w_{ij}}$$
Descomponiendo por la regla de la cadena:
$$\frac{\partial Loss}{\partial w_{ij}} = \frac{\partial Loss}{\partial out_j} \cdot \frac{\partial out_j}{\partial net_j} \cdot \frac{\partial net_j}{\partial w_{ij}}$$

## 3. Redes Neuronales Convolucionales (CNN)
Dise帽adas para procesar datos con estructura de rejilla (im谩genes).
- **Capas de Convoluci贸n**: Utilizan filtros (kernels) para extraer caracter铆sticas espaciales (bordes, texturas).
- **Capas de Pooling (Submuestreo)**: Reducen la resoluci贸n espacial, aportando invariancia a peque帽as traslaciones y reduciendo par谩metros.

## 4. Redes Neuronales Recurrentes (RNN)
Dise帽adas para datos secuenciales (texto, audio, series temporales).
- Poseen conexiones que retroalimentan la informaci贸n de pasos anteriores (**Memoria**).
- **LSTM (Long Short-Term Memory)**: Variante avanzada que soluciona el problema de la memoria a largo plazo mediante "puertas" (gates).

---
##  Ejercicio T茅cnico: C谩lculo de Salida
**Enunciado**: Dada una neurona con entradas $x_1=0.5, x_2=0.8$, pesos $w_1=0.2, w_2=0.4$ y sesgo (bias) $b=-0.1$. Calcule la salida si se usa una funci贸n de activaci贸n ReLU.

**Resoluci贸n**:
1. Suma ponderada: $z = (0.5 \cdot 0.2) + (0.8 \cdot 0.4) - 0.1$
2. $z = 0.1 + 0.32 - 0.1 = 0.32$
3. Activaci贸n: $f(0.32) = \max(0, 0.32) = 0.32$

---
> [!CAUTION]
> **Desvanecimiento de Gradiente (Vanishing Gradient)**: Ocurre cuando los gradientes se vuelven muy peque帽os al retropropagarse por muchas capas, impidiendo que los pesos de las primeras capas se actualicen. El uso de ReLU y arquitecturas como ResNet mitigan este problema.
