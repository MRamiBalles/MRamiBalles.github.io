# AA - Relaci칩n 3: 츼rboles de Decisi칩n y M칠todos de Ensamblado (Oficial UHU)

Los 치rboles de decisi칩n son modelos de aprendizaje supervisado no param칠tricos que utilizan una estructura jer치rquica de nodos para tomar decisiones basadas en las caracter칤sticas de los datos.

## 1. Construcci칩n del 츼rbol: Criterios de Divisi칩n
El objetivo es dividir el dataset de forma que cada partici칩n sea lo m치s "pura" posible respecto a la variable objetivo.

### Ganancia de Informaci칩n (Entrop칤a)
La entrop칤a mide el desorden en un conjunto de datos.
$$H(S) = - \sum_{i=1}^{c} p_i \log_2 p_i$$
La **Ganancia de Informaci칩n** es la reducci칩n de entrop칤a tras realizar una divisi칩n:
$$IG(S, A) = H(S) - \sum_{v \in Values(A)} \frac{|S_v|}{|S|} H(S_v)$$

### 칈ndice de Gini
Utilizado por el algoritmo CART, mide la probabilidad de clasificar err칩neamente un elemento elegido al azar.
$$Gini = 1 - \sum p_i^2$$

## 游닇 Ejercicio T칠cnico: C치lculo de Entrop칤a
**Escenario**: Dataset con 14 muestras: 9 "S칤" (Jugar al tenis) y 5 "No".
**Pregunta**: Calcule la entrop칤a inicial del sistema.

**Resoluci칩n**:
$p(+) = 9/14$, $p(-) = 5/14$
$H(S) = - (9/14 \log_2 9/14 + 5/14 \log_2 5/14) \approx 0.940$ bits.

---

## 2. El Problema del Sobreajuste: Podado (Pruning)
Los 치rboles tienden a crecer hasta memorizar el ruido.
- **Pre-pruning**: Detener el crecimiento si el n칰mero de muestras en un nodo es muy bajo.
- **Post-pruning**: Cortar ramas que no aportan una mejora significativa en la precisi칩n de validaci칩n.

## 3. M칠todos de Ensamblado (Ensemble)
Combinan m칰ltiples modelos d칠biles para crear uno fuerte.

### Bagging (Random Forest)
- Entrena m칰ltiples 치rboles de forma independiente usando muestras aleatorias con reemplazo (**Bootstrap**).
- Introduce aleatoriedad en la selecci칩n de atributos en cada nodo.
- Resultado: Promedio (regresi칩n) o Votaci칩n (clasificaci칩n).

### Boosting (XGBoost, AdaBoost)
- Entrena modelos de forma secuencial.
- Cada nuevo modelo intenta corregir los errores de los anteriores asignando m치s peso a las muestras mal clasificadas.

---
> [!IMPORTANT]
> **Interpretabilidad**: Una gran ventaja de los 치rboles individuales es su alta interpretabilidad ("Caja Blanca"). Sin embargo, los modelos de ensamblado suelen comportarse como "Cajas Negras", ofreciendo mucha m치s precisi칩n a cambio de perder la capacidad de visualizar la decisi칩n f치cilmente.
