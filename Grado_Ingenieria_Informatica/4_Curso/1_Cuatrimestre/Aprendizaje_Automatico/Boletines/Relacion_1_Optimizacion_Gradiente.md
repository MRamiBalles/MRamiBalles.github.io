# AA - Relaci贸n 1: Optimizaci贸n y Descenso de Gradiente (Oficial UHU)

El aprendizaje autom谩tico se fundamenta en la b煤squeda de los par谩metros de un modelo que minimizan una funci贸n de p茅rdida definida sobre un conjunto de datos.

## 1. Funci贸n de Coste ($J(\theta)$)
Mide el error entre la predicci贸n del modelo y el valor real.
- **Error Cuadr谩tico Medio (MSE)**: Utilizado en regresi贸n. $J(\theta) = \frac{1}{2m} \sum (h_\theta(x^{(i)}) - y^{(i)})^2$.

## 2. Descenso de Gradiente (Gradient Descent)
Algoritmo iterativo para encontrar el m铆nimo global de una funci贸n convexa.
- **Regla de Actualizaci贸n**: $\theta_j := \theta_j - \alpha \frac{\partial}{\partial \theta_j} J(\theta)$, donde $\alpha$ es la **Tasa de Aprendizaje** (learning rate).
- **Tipos**:
  - **Batch**: Usa todos los datos por iteraci贸n. Estable pero lento.
  - **Stochastic (SGD)**: Usa un 煤nico dato aleatorio por iteraci贸n. R谩pido pero ruidoso.
  - **Mini-batch**: Equilibrio entre ambos.

##  Ejercicio T茅cnico: C谩lculo de Gradiente
Derive la regla de actualizaci贸n para una regresi贸n lineal con una variable.
*Resoluci贸n*: 
- Sea $h_\theta(x) = \theta_0 + \theta_1 x$.
- $\frac{\partial J}{\partial \theta_0} = \frac{1}{m} \sum (h_\theta(x^{(i)}) - y^{(i)})$
- $\frac{\partial J}{\partial \theta_1} = \frac{1}{m} \sum (h_\theta(x^{(i)}) - y^{(i)}) \cdot x^{(i)}$

## 3. Problemas de Convergencia
- **$\alpha$ muy peque帽a**: Convergencia excesivamente lenta.
- **$\alpha$ muy grande**: Puede sobrepasar el m铆nimo e incluso divergir.
- **Normalizaci贸n**: Escalar los atributos (0-1) es cr铆tico para que el descenso de gradiente sea eficiente y no se estanque en valles el铆pticos.
