# AA - Teor铆a 1: Optimizaci贸n y Redes Neuronales Profundas (Oficial UHU)

El Aprendizaje Autom谩tico (ML) actual se fundamenta en la optimizaci贸n de funciones de p茅rdida mediante algoritmos de gradiente en arquitecturas neuronales complejas.

## 1. El Perceptr贸n Multicapa (MLP)
Extensi贸n del perceptr贸n simple que permite resolver problemas no linealmente separables (ej. XOR) mediante la introducci贸n de capas ocultas y funciones de activaci贸n no lineales (ReLU, Sigmoide, Tanh).

## 2. Algoritmo de Retropropagaci贸n (Backpropagation)
Es el mecanismo fundamental para el entrenamiento. Utiliza la regla de la cadena para calcular el gradiente de la funci贸n de p茅rdida con respecto a cada peso de la red, permitiendo la actualizaci贸n de los mismos mediante Descenso de Gradiente Estoc谩stico (SGD).

## 3. Regularizaci贸n y Sobreajuste (Overfitting)
Capacidad de la red para generalizar a datos no vistos.
- **L1/L2 Regularization**: A帽ade una penalizaci贸n a la magnitud de los pesos.
- **Dropout**: Desactiva neuronas aleatoriamente durante el entrenamiento para evitar co-dependencias.

## 4. Deep Learning Avanzado
- **Redes Convolucionales (CNN)**: Especializadas en datos espaciales (im谩genes).
- **Redes Recurrentes (RNN/LSTM)**: Especializadas en datos secuenciales (texto, series temporales).

##  Formulaci贸n T茅cnica
La funci贸n de p茅rdida para una regresi贸n log铆stica multiclasa (Softmax) viene dada por la Entrop铆a Cruzada:
$$L = -\sum y_i \log(\hat{y}_i)$$
Donde $\hat{y}_i$ es la probabilidad predicha para la clase $i$.

*Pregunta Cr铆tica*: 驴Por qu茅 la funci贸n ReLU ayuda a mitigar el problema del "Vanishing Gradient" en redes muy profundas en comparaci贸n con la Sigmoide?
*Respuesta*: Porque la derivada de ReLU es 1 para valores positivos, permitiendo que el gradiente fluya sin disminuir exponencialmente capa tras capa, a diferencia de la Sigmoide cuya derivada m谩xima es 0.25.
