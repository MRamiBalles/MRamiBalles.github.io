# SI - Relaci贸n 1: Aprendizaje por Refuerzo y Redes Neuronales (Oficial UHU)

Sistemas Inteligentes profundiza en algoritmos donde el agente aprende de su propia experiencia (Refuerzo) o de grandes vol煤menes de datos (Supervizado).

## 1. Aprendizaje por Refuerzo: Q-Learning
El agente aprende una pol铆tica 贸ptima mediante recompensas y castigos. La clave es la **Tabla Q(estado, acci贸n)**.

###  Ejercicio T茅cnico: Trazado de Q-Learning
**Enunciado**: Un agente est谩 en el estado $S_1$ y tiene dos acciones: $A$ (recompensa 0) y $B$ (recompensa 10). Si $\gamma = 0.9$ (factor de descuento) y $\alpha = 0.1$ (tasa de aprendizaje), calcule el nuevo valor de $Q(S_1, B)$ tras ejecutar la acci贸n B, asumiendo que el valor inicial era 0.

**Ecuaci贸n**: $Q(s, a) = Q(s, a) + \alpha [R + \gamma \max Q(s', a') - Q(s, a)]$
*Resoluci贸n*: 
- $Q(S_1, B) = 0 + 0.1 \cdot [10 + 0.9 \cdot 0 - 0] = 1$.
**Resultado**: El valor de la acci贸n B ha subido a 1, indicando al agente que es una buena decisi贸n.

## 2. Redes Neuronales: El Perceptr贸n
Es la unidad b谩sica de una red neuronal. Calcula la suma ponderada de sus entradas y aplica una funci贸n de activaci贸n (ej. Sigmoide o ReLU).

###  Ejercicio de Examen: L贸gica con Neuronas
**Enunciado**: Dise帽e un perceptr贸n que simule una puerta **AND**.
- Pesos: $w_1 = 1$, $w_2 = 1$.
- Umbral (Bias): $\theta = 1.5$.
- Funci贸n: $1$ si $\sum w_i x_i > \theta$, else $0$.

**Verificaci贸n**:
- Entrada (0,0): $0 < 1.5 \implies 0$.
- Entrada (1,0): $1 < 1.5 \implies 0$.
- Entrada (1,1): $2 > 1.5 \implies 1$ (Correcto).

## 3. Algoritmos Gen茅ticos
Inspirados en la evoluci贸n natural: Selecci贸n, Cruce y Mutaci贸n.
- **Cromosoma**: Codificaci贸n de la soluci贸n.
- **Fitness (Aptitud)**: Qu茅 tan buena es la soluci贸n.

---
> [!IMPORTANT]
> **Exploraci贸n vs Explotaci贸n**: En Q-Learning, la pol铆tica $\epsilon$-greedy permite al agente elegir a veces acciones al azar (exploraci贸n) para no quedarse atrapado en una soluci贸n sub贸ptima (explotaci贸n).
